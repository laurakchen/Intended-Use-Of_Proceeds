test.pred = predict(glm.fit, newx=test.mat[,-19])
train.errors.r[i] = mean((train$Salary[1:i]-train.pred)^2)
test.errors.r[i] = mean((test$Salary-test.pred)^2)
}
plot(seq(1:75), train.errors, ylim=c(0,2000000), type="l")
lines(seq(1:75), test.errors, type="l")
lines(seq(1:75), train.errors.r, type="l")
lines(seq(1:75), test.errors.r, type="l")
plot(seq(1:75), train.errors, ylim=c(0,2000000), type="l")
lines(seq(1:75), test.errors, type="l", col="red")
lines(seq(1:75), train.errors.r, type="l", col="green")
lines(seq(1:75), test.errors.r, type="l", col="blue")
plot(seq(1:75), train.errors, ylim=c(0,2000000),
xlim=c(20,75) type="l")
lines(seq(1:75), test.errors, type="l", col="red")
lines(seq(1:75), train.errors.r, type="l", col="green")
lines(seq(1:75), test.errors.r, type="l", col="blue")
plot(seq(1:75), train.errors, ylim=c(0,2000000),
xlim=c(20,75), type="l")
lines(seq(1:75), test.errors, type="l", col="red")
lines(seq(1:75), train.errors.r, type="l", col="green")
lines(seq(1:75), test.errors.r, type="l", col="blue")
plot(seq(1:75), train.errors, ylim=c(0,2000000),
xlim=c(20,75), type="l", xlab="Sample size",
ylab="Mean Squared Error", main="Mean Squared Error Vs. Sample Size")
lines(seq(1:75), test.errors, type="l", col="red")
lines(seq(1:75), train.errors.r, type="l", col="green")
lines(seq(1:75), test.errors.r, type="l", col="blue")
egend(60, 1500000, legend=c("Train-Least Squares", "Test-Least Squares",
"Train-Ridge", "Test-Ridge"),
col=c("purple", "red", "green", "blue"), lty=c(1,1,1,1))
legend(60, 1500000, legend=c("Train-Least Squares", "Test-Least Squares",
"Train-Ridge", "Test-Ridge"),
col=c("purple", "red", "green", "blue"), lty=c(1,1,1,1))
legend(40, 1500000, legend=c("Train-Least Squares", "Test-Least Squares",
"Train-Ridge", "Test-Ridge"),
col=c("purple", "red", "green", "blue"), lty=c(1,1,1,1))
plot(seq(1:75), train.errors, ylim=c(0,2000000),
xlim=c(20,75), type="l", xlab="Sample size",
ylab="Mean Squared Error", main="Mean Squared Error Vs. Sample Size",
col="purple")
lines(seq(1:75), test.errors, type="l", col="red")
lines(seq(1:75), train.errors.r, type="l", col="green")
lines(seq(1:75), test.errors.r, type="l", col="blue")
legend(50, 1500000, legend=c("Train-Least Squares", "Test-Least Squares",
"Train-Ridge", "Test-Ridge"),
col=c("purple", "red", "green", "blue"), lty=c(1,1,1,1))
train.errors.r
test.errors.r
# ridge plots
plot(seq(1:75), train.errors.r, ylim=c(0,400000), type="l",
main="Mean Squared Error Vs Sample Size
Using Ridge Regression",
xlab="Sample Size", ylab="Mean Squared Error")
points(seq(1:75), test.errors.r, col="red", type="l")
# ridge plots
plot(seq(1:75), train.errors.r, ylim=c(0,400000), type="l",
main="Mean Squared Error Vs Sample Size
Using Ridge Regression",
xlab="Sample Size", ylab="Mean Squared Error", xlim=c(20,75))
points(seq(1:75), test.errors.r, col="red", type="l")
legend(50, 300000, legend=c("Train", "Test"), col=c("black", "red"),
lty=c(1,1))
legend(60, 350000, legend=c("Train", "Test"), col=c("black", "red"),
lty=c(1,1))
train.errors
test.errors
plot(seq(1:75), train.errors, ylim=c(0,1700000), type="l",
main="Mean Squared Error Vs Sample Size
Using Least Squares",
xlab="Sample Size", ylab="Mean Squared Error", xlim=c(20,75))
points(seq(1:75), test.errors, col="red", type="l")
legend(60, 1600000, legend=c("Train", "Test"), col=c("black", "red"),
lty=c(1,1))
test.errors
plot(seq(1:75), train.errors, ylim=c(0,2200000),
xlim=c(20,75), type="l", xlab="Sample size",
ylab="Mean Squared Error", main="Mean Squared Error Vs. Sample Size",
col="purple")
lines(seq(1:75), test.errors, type="l", col="red")
lines(seq(1:75), train.errors.r, type="l", col="green")
lines(seq(1:75), test.errors.r, type="l", col="blue")
legend(50, 1500000, legend=c("Train-Least Squares", "Test-Least Squares",
"Train-Ridge", "Test-Ridge"),
col=c("purple", "red", "green", "blue"), lty=c(1,1,1,1))
glm.fit = glmnet(data.matrix(train[,-19]), train$Salary, alpha=0)
plot(glm.fit, xvar="lambda")
plot(glm.fit, xvar="lambda", label=TRUE)
plot(glm.fit, xvar="lambda", label=TRUE,
main="Coefficients Vs Log Lambda Using Ridge")
lasso.fit = glmnet(data.matrix(train[,-19]), train$Salary, alpha=1)
plot(lasso.fit, xvar="lambda", label=TRUE,
main="Coefficients Vs Log Lambda Using Lasso")
ridge.fit = glmnet(data.matrix(test[,-19]), test$Salary, alpha=0)
plot(ridge.fit)
ridge.fit = cv.glmnet(data.matrix(test[,-19]), test$Salary, alpha=0)
plot(ridge.fit)
plot(ridge.fit, main="Mean Squared Error Vs. Log Lambda Using Ridge")
lasso.fit = cv.glmnet(data.matrix(test[,-19]), test$Salary, alpha=1)
plot(lasso.fit, main="Mean Squared Error Vs. Log Lambda Using Lasso")
plot(ridge.fit, main="Mean Squared Error Vs. Log Lambda Using Ridge")
plot(lasso.fit, main="Mean Squared Error Vs. Log Lambda Using Lasso")
glm.fit = glmnet(data.matrix(train[,-19]), train$Salary, alpha=0)
plot(glm.fit, xvar="lambda", label=TRUE,
main="Coefficients Vs Log Lambda Using Ridge")
plot(lasso.fit, xvar="lambda", label=TRUE,
main="Coefficients Vs Log Lambda Using Lasso")
lasso.fit = glmnet(data.matrix(train[,-19]), train$Salary, alpha=1)
plot(lasso.fit, xvar="lambda", label=TRUE,
main="Coefficients Vs Log Lambda Using Lasso")
glm.fit = glmnet(data.matrix(train[,-19]), train$Salary, alpha=0)
plot(glm.fit, xvar="lambda", label=TRUE,
main="Coefficients Vs Log Lambda Using Ridge")
ridge.fit = cv.glmnet(data.matrix(test[,-19]), test$Salary, alpha=0)
plot(ridge.fit, main="Mean Squared Error Vs. Log Lambda Using Ridge")
lasso.fit = cv.glmnet(data.matrix(test[,-19]), test$Salary, alpha=1)
plot(lasso.fit, main="Mean Squared Error Vs. Log Lambda Using Lasso")
library(graphics)
df = UCBAdmissions
margin.table(df)
margin.table(df, c(1,2))
table(df)
table(df, c(1,2))
margin.table(df, margin=c(1,2))
margin.table(df, margin=1)
proportions(margin.table(df, margin=c(1,2)), "Gender")
proportions(df, margin=c(1,2))
xtabs(Freq ~ Gender + Admit, df)
prop.table(df)
prop.table(df, margin=1)
prop.table(df, margin=c(1,2))
prop.table(df, margin=2)
prop.table(margin.table(df, margin=c(1,2)), 1)
prop.table(margin.table(df, margin=c(1,2)), 2)
prop.table(margin.table(df, margin=1), 2)
prop.table(margin.table(df, margin=2), 2)
margin.table(df, margin=c(1,2))
prop.table(margin.table(df, margin=c(1,2)), 2)
marginal.tbl = margin.table(df, margin=c(1,2))
prop.table(marginal.tbl, 2) # proportions table
prop.table(df)
prop.table(df, 2)
prop.table(df, 1)
prop.table(df, c(1,2))
rm(list=ls())
library(MASS)
cats$Sex = as.factor(cats$Sex)
mod = lm(Hwt~0+Bwt*Sex, data=cats)
summary(mod)
anova(mod)
anova(lm(Hwt~0+Bwt:Sex, data=cats))
anova(lm(Hwt~0+Bwt+Sex+Bwt:Sex, data=cats))
anova(lm(Hwt~0+Bwt+Sex+Bwt:SexF, data=cats))
mod = lm(Hwt~0+Bwt:Sex, data=cats)
summary(mod)
mod2 = lm(Hwt~0+Bwt*Sex, data=cats)
anova(mod2)
summary(mod)
(mod$coefficients[3]-mod$coefficients[2])^2
mod$coefficients
(mod$coefficients[2]-mod$coefficients[1])^2
mod$coefficients[2]
mod$coefficients[2]-mod$coefficients[1]
3.914610-3.883453
(mod$coefficients[2]-mod$coefficients[1])^2
View(cats)
residuals(mod)
summary(lm(Hwt~Bwt+Sex*cats$Bwt[cats$Sex=="F"],data=cats))
set.seed(1)
resample = function(n) {
return(sample(n, size=length(n), replace=TRUE))
}
f.resids = residuals(mod)[1:47]
m.resids = residuals(mod)[48:144]
B = 1000
t.star = rep(NA, B)
coef.F = rep(NA, B)
coef.M = rep(NA, B)
for (i in 1:B) {
f.noise = resample(f.resids)
m.noise = resample(m.resids)
new.f = data.frame(Bwt=cats$Bwt[cats$Sex == "F"],
Hwt=fitted(mod)[1:47]+f.noise,
Sex="F")
new.m = data.frame(Bwt=cats$Bwt[cats$Sex == "M"],
Hwt=fitted(mod)[48:144]+m.noise,
Sex="M")
new = rbind(new.f, new.m)
coef.f = lm(Hwt~Bwt:Sex, data=new)$coefficients[1]
coef.m = lm(Hwt~Bwt:Sex, data=new)$coefficients[2]
t.star[i] = (coef.m-coef.f)^2
}
hist(t.star, main="Distribution of T*")
abline(v=test.stat, col="purple")
test.stat = (mod$coefficients[2]-mod$coefficients[1])^2
test.stat
hist(t.star, main="Distribution of T*")
abline(v=test.stat, col="purple")
hist(t.star, main="Distribution of T*", xlab="T*")
abline(v=test.stat, col="purple")
mean(t.star >= test.stat)
set.seed(1)
resample = function(n) {
return(sample(n, size=length(n), replace=TRUE))
}
B = 1000
t.star = rep(NA, B)
for (i in 1:B) {
f.noise = resample(residuals(mod)[1:47])
m.noise = resample(residuals(mod)[48:144])
new.f = data.frame(Bwt=cats$Bwt[cats$Sex == "F"],
Hwt=fitted(mod)[1:47]+f.noise,
Sex="F")
new.m = data.frame(Bwt=cats$Bwt[cats$Sex == "M"],
Hwt=fitted(mod)[48:144]+m.noise,
Sex="M")
new = rbind(new.f, new.m)
coef.f = lm(Hwt~0+Bwt:Sex, data=new)$coefficients[1]
coef.m = lm(Hwt~0+Bwt:Sex, data=new)$coefficients[2]
t.star[i] = (coef.m-coef.f)^2
}
hist(t.star, main="Distribution of T*", xlab="T*")
abline(v=test.stat, col="purple")
mean(t.star >= test.stat)
summary(lm(Hwt~0+Bwt:Sex, data=cats))
mod$coefficients[1]
(cats$Sex=="F")
residuals(mod)[1:47]
residuals(mod)
sum((cats$Sex=="F"))
sum((cats$Sex=="M"))
residuals(mod)[sample(47, size=47, replace=TRUE)]
set.seed(1)
resample = function(n) {
return(sample(n, size=length(n), replace=TRUE))
}
B = 1000
t.star = rep(NA, B)
for (i in 1:B) {
f.noise = residuals(mod)[sample(47, size=47, replace=TRUE)]
m.noise = residuals(mod)[sample(97, size=97, replace=TRUE)]
#f.noise = resample(residuals(mod)[1:47])
# m.noise = resample(residuals(mod)[48:144])
new.f = data.frame(Bwt=cats$Bwt[cats$Sex == "F"],
Hwt=fitted(mod)[1:47]+f.noise,
Sex="F")
new.m = data.frame(Bwt=cats$Bwt[cats$Sex == "M"],
Hwt=fitted(mod)[48:144]+m.noise,
Sex="M")
new = rbind(new.f, new.m)
coef.f = lm(Hwt~0+Bwt:Sex, data=new)$coefficients[1]
coef.m = lm(Hwt~0+Bwt:Sex, data=new)$coefficients[2]
t.star[i] = (coef.m-coef.f)^2
}
hist(t.star, main="Distribution of T*", xlab="T*")
abline(v=test.stat, col="purple")
mean(t.star >= test.stat)
cats$Bwt[cats$Sex == "F"]
cats[cats$Sex == "F"]
cats[cats$Sex == "F",]
mod.f = lm(Hwt~0+Bwt, data=cats[cats$Sex == "F",])
residuals(mod.f)
mod.m = lm(Hwt~0+Bwt, data=cats[cats$Sex == "M",])
residuals(mod.m)
resample(residuals(mod.m))
set.seed(1)
resample = function(n) {
return(sample(n, size=length(n), replace=TRUE))
}
B = 1000
t.star = rep(NA, B)
mod.f = lm(Hwt~0+Bwt, data=cats[cats$Sex == "F",])
mod.m = lm(Hwt~0+Bwt, data=cats[cats$Sex == "M",])
for (i in 1:B) {
f.noise = resample(residuals(mod.f))
m.noise = resample(residuals(mod.m))
new.f = data.frame(Bwt=cats$Bwt[cats$Sex == "F"],
Hwt=fitted(mod.f)+f.noise,
Sex="F")
new.m = data.frame(Bwt=cats$Bwt[cats$Sex == "M"],
Hwt=fitted(mod.m)+m.noise,
Sex="M")
new = rbind(new.f, new.m)
coef.f = lm(Hwt~0+Bwt:Sex, data=new)$coefficients[1]
coef.m = lm(Hwt~0+Bwt:Sex, data=new)$coefficients[2]
t.star[i] = (coef.m-coef.f)^2
}
hist(t.star, main="Distribution of T*", xlab="T*")
abline(v=test.stat, col="purple")
mean(t.star >= test.stat)
library(mgcv)
gam(dist ~ s(speed, k = 4 + 1, fx = TRUE), data = cars)
gam(Sepal.Length ~ s(Petal.Length, by = Species, k = 4 + 1, fx = TRUE),
data = iris)
anova(fit1, fit2, test = "F")
update.packages(ask = FALSE, checkBuilt = TRUE)
tinytex::tlmgr_update()
tinytex::reinstall_tinytex()
tinytex::reinstall_tinytex()
install.packages("xfun")
install.packages("xfun")
tinytex::reinstall_tinytex()
setwd("~/desktop/FinProject")
# load data
library(readxl)
df = read_xlsx("Analysis/IPO_data.xlsx")
df = subset(df, select=-c(search_avail, search_vol_avg,
original_high_filing_price,original_low_filing_price,
upward_adjustment, offer_price, closing_price))
df = df[!duplicated(df), ] # drop duplicates
# use min-max scaling to normalize data
library(caret)
df_numeric = df[sapply(df,is.numeric)]
preproc2 <- preProcess(df_numeric, method=c("range"))
norm2 <- predict(preproc2, df_numeric)
norm2 = norm2[!(norm2$underpricing == 0),] # get rid of zero value in underpricing
# OLS with log(x+1) transformation on underpricing // use in paper
df_trans = norm2
df_trans$underpricing = log1p(df_trans$underpricing)
model_trans = lm(underpricing~., data=df_trans)
summary(model_trans)
#QQ plot
qqnorm(model_trans$residuals)
qqline(model_trans$residuals)
plot(residuals(model_trans)~fitted(model_trans))
library(MASS)
bc = boxcox(underpricing~., data=df_trans)
lambda <- bc$x[which.max(bc$y)]
new_model <- lm(((underpricing^lambda-1)/lambda) ~., data=df_trans)
summary(new_model)
plot(residuals(new_model)~fitted(new_model))
#Q-Q plot for Box-Cox transformed model
qqnorm(new_model$residuals)
qqline(new_model$residuals)
new = df_trans
new$proceeds_amt_mil = log1p(new$proceeds_amt_mil)
new$primary_shares_offered = log1p(new$primary_shares_offered)
new$secondary_shares_offered = log1p(new$secondary_shares_offered)
new$c3 = log1p(new$c3)
new$c4 = log1p(new$c4)
new$uncertainty = log1p(new$uncertainty)
new$litigious = log1p(new$litigious)
new$strongmodal = log1p(new$strongmodal)
new$weakmodal = log1p(new$weakmodal)
new$constraining = log1p(new$constraining)
# boxcox transformation on log transformed response variable and log transformed
# predictor variables
bc_new = boxcox(underpricing~., data=new)
lambda_new <- bc_new$x[which.max(bc_new$y)]
new_model_new <- lm(((underpricing^lambda_new-1)/lambda_new) ~., data=new)
summary(new_model_new)
plot(residuals(new_model_new)~fitted(new_model_new))
qqnorm(residuals(new_model_new))
qqline(residuals(new_model_new))
hist(residuals(new_model_new))
hist(new_model$residuals)
bc_gam = gam(formula=((underpricing^lambda-1)/lambda)~venture_backed+
s(num_bookrunners)+s(rank_no_leads)+
s(num_lead_colead_managers)+s(c1)+s(c2)+s(c3)+s(c4)+
s(word_length_sentiment)+s(negative)+s(positive)+
s(uncertainty)+s(litigious)+s(strongmodal)+s(weakmodal)+
s(constraining)+internet+s(nasdaq_returns)+s(vix_returns),
data=df_trans)
# boxcox transformation
# (source: https://www.statology.org/box-cox-transformation-in-r/)
library(MASS)
gam_mod_new = gam(formula=underpricing~
venture_backed+s(num_bookrunners)+s(rank_no_leads)+
s(num_lead_colead_managers)+s(c1)+s(c2)+s(c3)+s(c4)+
s(word_length_sentiment)+s(negative)+s(positive)+
s(uncertainty)+s(litigious)+s(strongmodal)+s(weakmodal)+
s(constraining)+internet+s(nasdaq_returns)+s(vix_returns),
data=new)
library(mgcv)
bc_gam = gam(formula=((underpricing^lambda-1)/lambda)~venture_backed+
s(num_bookrunners)+s(rank_no_leads)+
s(num_lead_colead_managers)+s(c1)+s(c2)+s(c3)+s(c4)+
s(word_length_sentiment)+s(negative)+s(positive)+
s(uncertainty)+s(litigious)+s(strongmodal)+s(weakmodal)+
s(constraining)+internet+s(nasdaq_returns)+s(vix_returns),
data=df_trans)
summary(bc_gam)
plot(residuals(bc_gam)~fitted(bc_gam))
plot(bc_gam)
plot(residuals(bc_gam)~fitted(bc_gam))
gam.check(bc_gam)
plot(residuals(bc_gam)~fitted(bc_gam))
qqnorm(residuals(bc_gam))
qqline(residuals(bc_gam))
bc_gam_1 = gam(formula=((underpricing^lambda_new-1)/lambda_new)~venture_backed+
s(num_bookrunners)+s(rank_no_leads)+
s(num_lead_colead_managers)+s(c1)+s(c2)+s(c3)+s(c4)+
s(word_length_sentiment)+s(negative)+s(positive)+
s(uncertainty)+s(litigious)+s(strongmodal)+s(weakmodal)+
s(constraining)+internet+s(nasdaq_returns)+s(vix_returns),
data=new)
summary(bc_gam_1)
plot(residuals(bc_gam_1)~fitted(bc_gam_1))
qqnorm(residuals(bc_gam_1))
qqline(residuals(bc_gam_1))
qqnorm(residuals(bc_gam))
qqline(residuals(bc_gam))
summary(bc_gam)
#Q-Q plot for Box-Cox transformed model
qqnorm(new_model$residuals)
qqline(new_model$residuals)
hist(bc_gam$residuals)
summary(new_model)
devtools::install_github("tysonstanley/dissertateUSU"
devtools::install_github("tysonstanley/dissertateUSU")
setwd("~/desktop/FinProject")
df = read_xlsx("Analysis/IPO_data.xlsx")
kable(summary(model_trans))
library(knitr)
kable(summary(model_trans))
tidy(model_trans)
?tidy
kable(tidy(summary(model_trans)))
install.packages("broom")
library(broom)
kable(tidy(summary(model_trans)))
pairs(df)
pairs(norm2)
summary(model_trans)
summary(df_trnas)
summary(df_trans)
kable(tidy(summary(df_trans)))
kable(summary(df_trans))
length(norm2)
View(df_trans)
View(df)
cor(df_trans)
hist(norm2$underpricing)
hist(df_trans$underpricing)
install.packages("gtsummary")
library(gtsummary)
tbl_summary(df_trans)
hist(df$underpricing)
View(df)
kable(cor(df_trans))
install.packages(pastecs)
install.packages("pastecs")
library(pastecs)
stat.desc(df_trans)
stat.desc(df_trans, basic=F)
summary_table(df_trans)
librayr(dplyr)
library(dplyr)
summary.table(df_trans)
summary_table(df_trans)
table = stat.desc(df, basic=TRUE, desc=TRUE)
View(table)
df_numeric = df[sapply(df,is.numeric)]
table = stat.desc(df_numeric, basic=TRUE, desc=TRUE)
View(table)
tbl_summary(df)
View(table)
tbl_summary(df_numeric)
library(readxl)
library(knitr)
library(broom)
library(gtsummary)
library(pastecs)
df = read_xlsx("~/desktop/FinProject/Analysis/IPO_data.xlsx")
df = subset(df, select=-c(search_avail, search_vol_avg,
original_high_filing_price,original_low_filing_price,
upward_adjustment, offer_price, closing_price))
df = df[!duplicated(df), ] # drop duplicates
df_numeric = df[sapply(df,is.numeric)]
tbl_summary(df_numeric)
View(table)
summary(df$underpricing)
length(df_numeric)
summary(df_numeric)
df = df[!is.na(df$vix_returns),]
df_numeric = df[sapply(df,is.numeric)]
summary(df_numeric)
kable(summary(df_numeric))
library(pander)
pander(summary(df_numeric))
stat.desc(df_numeric, basic=FALSE)
apply(df_numeric,2,summary(x))
apply(df_numeric,2, function(x) summary(x))
table(apply(df_numeric,2, function(x) summary(x)))
t(apply(df_numeric,2, function(x) summary(x)))
kable(t(apply(df_numeric,2, function(x) summary(x))))
pander(t(apply(df_numeric,2, function(x) summary(x))), split.cells=6)
df %>%
rename(
c1 = acquisitons,
c2 = financing_transactions,
c3 = growth_invest,
c4 = production_invest
)
df %>%
rename(
acquisitions = c1,
financing_transactions = c2,
growth_invest = c3,
production_invest = c4
)
View(df)
df = df %>%
rename(
acquisitions = c1,
financing_transactions = c2,
growth_invest = c3,
production_invest = c4
)
df_numeric = df[sapply(df,is.numeric)]
pander(t(apply(df_numeric,2, function(x) summary(x))), split.cells=6)
hist(df_numeric$underpricing, xlab="Underpricing", main="")
